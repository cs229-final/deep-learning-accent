{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/leonmak/Envs/cs229-final/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN: Initial Data Points = 918554\n",
      "OUT: Total features = (88, 60, 41, 2)\n"
     ]
    }
   ],
   "source": [
    "def windows(data, window_size):\n",
    "    start = 0\n",
    "    while start < len(data):\n",
    "        yield start, start + window_size\n",
    "        start += (window_size // 2)\n",
    "\n",
    "def extract_feature_array(filename, bands=60, frames=41):\n",
    "    window_size = 512 * (frames-1)\n",
    "    log_specgrams = []\n",
    "    sound_clip,s = librosa.load(filename)        \n",
    "    for (start, end) in windows(sound_clip, window_size):\n",
    "        start = int(start)\n",
    "        end = int(end)\n",
    "        if(len(sound_clip[start:end]) == window_size):\n",
    "            signal = sound_clip[start:end]\n",
    "            melspec = librosa.feature.melspectrogram(signal, n_mels = bands)\n",
    "            logspec = librosa.logamplitude(melspec)\n",
    "            logspec = logspec.T.flatten()[:, np.newaxis].T\n",
    "            log_specgrams.append(logspec)\n",
    "            \n",
    "    log_specgrams = np.asarray(log_specgrams).reshape(len(log_specgrams),bands,frames,1)\n",
    "    features = np.concatenate((log_specgrams, np.zeros(np.shape(log_specgrams))), axis = 3)\n",
    "    for i in range(len(features)):\n",
    "        features[i, :, :, 1] = librosa.feature.delta(features[i, :, :, 0])\n",
    "    \n",
    "    return np.array(features)\n",
    "\n",
    "sample_filename = 'data/speech-accent-archive/recordings/afghanistan/dari2.mp3'\n",
    "features = extract_feature_array(sample_filename)\n",
    "data_points, _ = librosa.load(sample_filename)\n",
    "print ('IN: Initial Data Points =', len(data_points))\n",
    "print ('OUT: Total features =', np.shape(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "176 labels found, 1st 5: ['afghanistan', 'albania', 'algeria', 'andorra', 'angola']\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "def iter_label_files(parent_dir):\n",
    "    for root, dirpaths, fnames in os.walk(parent_dir):\n",
    "        if len(dirpaths) > 0:\n",
    "            continue\n",
    "        label = root.split('/')[-1]\n",
    "        yield root, label, fnames\n",
    "\n",
    "parent_dir = 'data/speech-accent-archive/recordings/'\n",
    "labels = []\n",
    "for _, label, _ in iter_label_files(parent_dir):\n",
    "    labels.append(label)\n",
    "print(f'{len(labels)} labels found, 1st 5: {labels[:5]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['canada', 'south_korea', 'brazil', 'belgium', 'turkey', 'poland']\n"
     ]
    }
   ],
   "source": [
    "cap = 0\n",
    "counts = []\n",
    "for _, label, fnames in iter_label_files(parent_dir):\n",
    "    counts.append(len(fnames))\n",
    "\n",
    "top_indices_count = [i_count for i_count in sorted(enumerate(counts), key=lambda x:x[1], reverse=True)][4:10]\n",
    "most_freq_egs = [labels[i] for (i, count) in top_indices_count if count >= cap]\n",
    "print(most_freq_egs)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving fold_0\n",
      "Features of fold_0 = (297, 60, 41, 2)\n",
      "Labels of fold_0 = (303,)\n",
      "Saved npy/spec/fold_0_x.npy\n",
      "Saved npy/spec/fold_0_y.npy\n",
      "\n",
      "Saving fold_1\n",
      "Features of fold_1 = (306, 60, 41, 2)\n",
      "Labels of fold_1 = (312,)\n",
      "Saved npy/spec/fold_1_x.npy\n",
      "Saved npy/spec/fold_1_y.npy\n",
      "\n",
      "Saving fold_2\n",
      "Features of fold_2 = (376, 60, 41, 2)\n",
      "Labels of fold_2 = (382,)\n",
      "Saved npy/spec/fold_2_x.npy\n",
      "Saved npy/spec/fold_2_y.npy\n",
      "\n",
      "Saving fold_3\n",
      "Features of fold_3 = (357, 60, 41, 2)\n",
      "Labels of fold_3 = (363,)\n",
      "Saved npy/spec/fold_3_x.npy\n",
      "Saved npy/spec/fold_3_y.npy\n",
      "\n",
      "Saving fold_4\n",
      "Features of fold_4 = (321, 60, 41, 2)\n",
      "Labels of fold_4 = (327,)\n",
      "Saved npy/spec/fold_4_x.npy\n",
      "Saved npy/spec/fold_4_y.npy\n",
      "\n",
      "Saving fold_5\n",
      "Features of fold_5 = (336, 60, 41, 2)\n",
      "Labels of fold_5 = (342,)\n",
      "Saved npy/spec/fold_5_x.npy\n",
      "Saved npy/spec/fold_5_y.npy\n",
      "\n",
      "Saving fold_6\n",
      "Features of fold_6 = (341, 60, 41, 2)\n",
      "Labels of fold_6 = (347,)\n",
      "Saved npy/spec/fold_6_x.npy\n",
      "Saved npy/spec/fold_6_y.npy\n",
      "\n",
      "Saving fold_7\n",
      "Features of fold_7 = (325, 60, 41, 2)\n",
      "Labels of fold_7 = (331,)\n",
      "Saved npy/spec/fold_7_x.npy\n",
      "Saved npy/spec/fold_7_y.npy\n",
      "\n",
      "Saving fold_8\n",
      "Features of fold_8 = (334, 60, 41, 2)\n",
      "Labels of fold_8 = (340,)\n",
      "Saved npy/spec/fold_8_x.npy\n",
      "Saved npy/spec/fold_8_y.npy\n",
      "\n",
      "Saving fold_9\n",
      "Features of fold_9 = (308, 60, 41, 2)\n",
      "Labels of fold_9 = (314,)\n",
      "Saved npy/spec/fold_9_x.npy\n",
      "Saved npy/spec/fold_9_y.npy\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random \n",
    "\n",
    "\n",
    "def get_label_enc(fn, le):\n",
    "    label_txt = fn.split('/')[-2]  # y_i\n",
    "    label_int = le.transform([label_txt])\n",
    "    return label_int[0]\n",
    "\n",
    "\n",
    "def extract_feature(fnames, le, bands=60, frames=41):\n",
    "    \"\"\"\n",
    "    Extract features from filenames to features and labels arrays\n",
    "    \"\"\"\n",
    "    window_size = 512 * (frames-1)\n",
    "    log_specgrams = []\n",
    "    labels = []\n",
    "    for fn in fnames:\n",
    "        X,s = librosa.load(fn)\n",
    "        label_enc = get_label_enc(fn, le)\n",
    "        for (start, end) in windows(X, window_size):\n",
    "            labels.append(label_enc)\n",
    "            curr_win_size = len(X[start:end])\n",
    "            if(curr_win_size != window_size):\n",
    "                break\n",
    "            signal = X[start:end]\n",
    "            melspec = librosa.feature.melspectrogram(signal, n_mels=bands)\n",
    "            logspec = librosa.logamplitude(melspec)\n",
    "            logspec = logspec.T.flatten()[:, np.newaxis].T\n",
    "            log_specgrams.append(logspec)\n",
    "    log_specgrams = np.array(log_specgrams).reshape(len(log_specgrams),bands,frames,1)\n",
    "    features = np.concatenate((log_specgrams, np.zeros(log_specgrams.shape)), axis=3)\n",
    "    for i in range(len(features)):\n",
    "        features[i, :, :, 1] = librosa.feature.delta(features[i, :, :, 0])\n",
    "    return np.array(features), np.array(labels, dtype='int')\n",
    "\n",
    "\n",
    "def get_subset_fnames(cap=10):\n",
    "    fnames = glob.glob('**/*.mp3', recursive=True)\n",
    "    subset_fnames = []\n",
    "    for root, label, fnames in iter_label_files(parent_dir):\n",
    "        if label not in most_freq_egs:\n",
    "            continue\n",
    "        for fn in fnames[:cap]:\n",
    "            subset_fnames.append(os.path.join(root, fn))\n",
    "    random.seed(1)\n",
    "    random.shuffle(subset_fnames)\n",
    "    return subset_fnames\n",
    "\n",
    "\n",
    "def save_folds(save_dir, le, num_folds=10):\n",
    "    fnames = get_subset_fnames()\n",
    "    egs_per_fold = len(fnames) // num_folds\n",
    "    curr = 0\n",
    "    for k in range(num_folds):\n",
    "        fold_name = 'fold_' + str(k)\n",
    "        \n",
    "        print('Saving ' + fold_name)\n",
    "        start_i = k * egs_per_fold\n",
    "        end_i = (k + 1) * egs_per_fold\n",
    "        fnames_fold = fnames[start_i: end_i]\n",
    "        features, labels = extract_feature(fnames_fold, le)\n",
    "        \n",
    "        print(f'Features of {fold_name} = {features.shape}')\n",
    "        print(f'Labels of {fold_name} = {labels.shape}')\n",
    "\n",
    "        feature_file = os.path.join(save_dir, fold_name + '_x.npy')\n",
    "        labels_file = os.path.join(save_dir, fold_name + '_y.npy')\n",
    "\n",
    "        random.seed(1)\n",
    "        random.shuffle(features)\n",
    "        random.shuffle(labels)\n",
    "\n",
    "        np.save(feature_file, features)\n",
    "        np.save(labels_file, labels)\n",
    "        print('Saved ' + feature_file)\n",
    "        print('Saved ' + labels_file + '\\n')\n",
    "        \n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(most_freq_egs)\n",
    "save_dir=os.path.join('npy', 'spec')\n",
    "save_folds(save_dir, le)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "npy/spec\n",
      "\n",
      "Adding fold_0\n",
      "New Features:  (297, 60, 41, 2)\n",
      "\n",
      "Adding fold_1\n",
      "New Features:  (306, 60, 41, 2)\n",
      "\n",
      "Adding fold_2\n",
      "New Features:  (376, 60, 41, 2)\n",
      "\n",
      "Adding fold_3\n",
      "New Features:  (357, 60, 41, 2)\n",
      "\n",
      "Adding fold_4\n",
      "New Features:  (321, 60, 41, 2)\n",
      "\n",
      "Adding fold_5\n",
      "New Features:  (336, 60, 41, 2)\n",
      "\n",
      "Adding fold_6\n",
      "New Features:  (341, 60, 41, 2)\n",
      "\n",
      "Training Set: (308, 60, 41, 2), Labels: (308, 6)\n",
      "Train-dev Set: (308, 60, 41, 2), Labels: (308, 6)\n",
      "Dev Set: (308, 60, 41, 2), Labels: (308, 6)\n",
      "Test Set: (308, 60, 41, 2), Labels: (308, 6)\n"
     ]
    }
   ],
   "source": [
    "data_dir = os.path.join('npy', 'spec')\n",
    "print(data_dir)\n",
    "\n",
    "def add_folds(data_dir):\n",
    "    num_folds = len(os.listdir(data_dir)) // 2\n",
    "        \n",
    "    for k in range(num_folds-3):\n",
    "        fold_name = 'fold_' + str(k)\n",
    "        print(\"\\nAdding \" + fold_name)\n",
    "        feature_file = os.path.join(data_dir, fold_name + '_x.npy')\n",
    "        labels_file = os.path.join(data_dir, fold_name + '_y.npy')\n",
    "        loaded_features = np.load(feature_file)\n",
    "        loaded_labels = np.load(labels_file)\n",
    "        print(\"New Features: \", loaded_features.shape)\n",
    "\n",
    "        if k > 0:\n",
    "            features = np.concatenate((features, loaded_features))\n",
    "            labels = np.concatenate((labels, loaded_labels))\n",
    "        else:\n",
    "            features = loaded_features\n",
    "            labels = loaded_labels\n",
    "        \n",
    "    return features, labels\n",
    "\n",
    "train_x, train_y = add_folds(data_dir)\n",
    "\n",
    "# use a fold for train-dev\n",
    "valid_fold_name = 'fold_7'\n",
    "feature_file = os.path.join(data_dir, valid_fold_name + '_x.npy')\n",
    "labels_file = os.path.join(data_dir, valid_fold_name + '_y.npy')\n",
    "train_dev_x = np.load(feature_file)\n",
    "train_dev_y = np.load(labels_file) \n",
    "\n",
    "# use a fold for dev\n",
    "valid_fold_name = 'fold_8'\n",
    "feature_file = os.path.join(data_dir, valid_fold_name + '_x.npy')\n",
    "labels_file = os.path.join(data_dir, valid_fold_name + '_y.npy')\n",
    "dev_x = np.load(feature_file)\n",
    "dev_y = np.load(labels_file) \n",
    "\n",
    "# and a fold for testing\n",
    "test_fold_name = 'fold_9'\n",
    "feature_file = os.path.join(data_dir, test_fold_name + '_x.npy')\n",
    "labels_file = os.path.join(data_dir, test_fold_name + '_y.npy')\n",
    "test_x = np.load(feature_file)\n",
    "test_y = np.load(labels_file)\n",
    "\n",
    "# # # One hot encode labels\n",
    "ohe = preprocessing.OneHotEncoder(sparse=False)\n",
    "ohe.fit(train_y.reshape(-1, 1))\n",
    "\n",
    "train_y = ohe.transform(train_y.reshape((-1, 1)))\n",
    "num_labels = len(train_y[1])\n",
    "train_dev_y = ohe.transform(train_dev_y.reshape((-1, 1)))\n",
    "dev_y = ohe.transform(dev_y.reshape((-1, 1)))\n",
    "test_y = ohe.transform(test_y.reshape((-1, 1)))\n",
    "\n",
    "# trim\n",
    "\n",
    "last = min([len(train_x), len(train_dev_x), len(dev_x), len(test_x)])\n",
    "train_x, train_y = train_x[:last], train_y[:last]\n",
    "train_dev_x, train_dev_y = train_dev_x[:last], train_dev_y[:last]\n",
    "dev_x, dev_y = dev_x[:last], dev_y[:last]\n",
    "test_x, test_y = test_x[:last], test_y[:last]\n",
    "\n",
    "print(f\"\\nTraining Set: {train_x.shape}, Labels: {train_y.shape}\")\n",
    "print(f\"Train-dev Set: {train_dev_x.shape}, Labels: {train_dev_y.shape}\")\n",
    "print(f\"Dev Set: {dev_x.shape}, Labels: {dev_y.shape}\")\n",
    "print(f\"Test Set: {test_x.shape}, Labels: {test_y.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.set_random_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "from keras.models import Sequential\n",
    "import keras.layers as lyers\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution2D, MaxPooling2D\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score\n",
    "from keras.regularizers import l2\n",
    "from keras.utils import np_utils\n",
    "\n",
    "frames = 41\n",
    "tfbands = 60\n",
    "num_channels = 2\n",
    "feature_size = bands * frames #60x4\n",
    "num_labels = 6\n",
    "input_shape=(bands, frames, num_channels)\n",
    "\n",
    "\n",
    "def build_model():\n",
    "    \n",
    "    model = Sequential()\n",
    "    # input: 60x41 data frames with 2 channels => (60,41,2) tensors\n",
    "\n",
    "    # filters of size 3x3 - paper describes using 5x5, but their input data is 128x128\n",
    "    f_size = 3\n",
    "\n",
    "    # Layer 1 - 24 filters with a receptive field of (f,f), i.e. W has the\n",
    "    # shape (24,1,f,f).  This is followed by (4,2) max-pooling over the last\n",
    "    # two dimensions and a ReLU activation function\n",
    "    model.add(Convolution2D(24, f_size, f_size, border_mode='same', input_shape=(bands, frames, num_channels)))\n",
    "    model.add(MaxPooling2D(pool_size=(4, 2)))\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    # Layer 2 - 48 filters with a receptive field of (f,f), i.e. W has the \n",
    "    # shape (48, 24, f, f). Like L1 this is followed by (4,2) max-pooling \n",
    "    # and a ReLU activation function.\n",
    "    model.add(Convolution2D(48, f_size, f_size, border_mode='same'))\n",
    "    model.add(MaxPooling2D(pool_size=(4, 2)))\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    # Layer 3 - 48 filters with a receptive field of (f,f), i.e. W has the\n",
    "    # shape (48, 48, f, f). This is followed by a ReLU but no pooling.\n",
    "    model.add(Convolution2D(48, f_size, f_size, border_mode='valid'))\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    # flatten output into a single dimension, let Keras do shape inference\n",
    "    model.add(Flatten())\n",
    "\n",
    "    # Layer 4 - a fully connected NN layer of 64 hidden units, L2 penalty of 0.001\n",
    "    model.add(Dense(64, W_regularizer=l2(0.001)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    # Layer 5 - an output layer with one output unit per class, with L2 penalty, \n",
    "    # followed by a softmax activation function\n",
    "    model.add(Dense(num_labels, W_regularizer=l2(0.001)))\n",
    "    model.add(Dropout(0.5))\n",
    "    # softmax on single output will always normalise the value to 1.0, so need sigmoid\n",
    "    model.add(Activation('sigmoid'))\n",
    "\n",
    "    # create an optimiser\n",
    "    # adagrad = Adagrad(lr=0.01, epsilon=1e-08, decay=0.0)\n",
    "    sgd = SGD(lr=0.001, momentum=0.9, decay=0.0, nesterov=True)\n",
    "\n",
    "    # compile and fit model, reduce epochs if you want a result faster\n",
    "    # the validation set is used to identify parameter settings (epoch) that achieves \n",
    "    # the highest classification accuracy (note binary rather than categorial crossentropy)\n",
    "    model.compile(loss='binary_crossentropy', metrics=['accuracy'], optimizer=sgd)\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/leonmak/Envs/cs229-final/lib/python3.6/site-packages/ipykernel_launcher.py:33: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(24, (3, 3), input_shape=(60, 41, 2..., padding=\"same\")`\n",
      "/Users/leonmak/Envs/cs229-final/lib/python3.6/site-packages/ipykernel_launcher.py:40: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(48, (3, 3), padding=\"same\")`\n",
      "/Users/leonmak/Envs/cs229-final/lib/python3.6/site-packages/ipykernel_launcher.py:46: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(48, (3, 3), padding=\"valid\")`\n",
      "/Users/leonmak/Envs/cs229-final/lib/python3.6/site-packages/ipykernel_launcher.py:53: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(64, kernel_regularizer=<keras.reg...)`\n",
      "/Users/leonmak/Envs/cs229-final/lib/python3.6/site-packages/ipykernel_launcher.py:59: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(6, kernel_regularizer=<keras.reg...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_112 (Conv2D)          (None, 60, 41, 24)        456       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_61 (MaxPooling (None, 15, 20, 24)        0         \n",
      "_________________________________________________________________\n",
      "activation_87 (Activation)   (None, 15, 20, 24)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_113 (Conv2D)          (None, 15, 20, 48)        10416     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_62 (MaxPooling (None, 3, 10, 48)         0         \n",
      "_________________________________________________________________\n",
      "activation_88 (Activation)   (None, 3, 10, 48)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_114 (Conv2D)          (None, 1, 8, 48)          20784     \n",
      "_________________________________________________________________\n",
      "activation_89 (Activation)   (None, 1, 8, 48)          0         \n",
      "_________________________________________________________________\n",
      "flatten_25 (Flatten)         (None, 384)               0         \n",
      "_________________________________________________________________\n",
      "dense_47 (Dense)             (None, 64)                24640     \n",
      "_________________________________________________________________\n",
      "activation_90 (Activation)   (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_70 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_48 (Dense)             (None, 6)                 390       \n",
      "_________________________________________________________________\n",
      "dropout_71 (Dropout)         (None, 6)                 0         \n",
      "_________________________________________________________________\n",
      "activation_91 (Activation)   (None, 6)                 0         \n",
      "=================================================================\n",
      "Total params: 56,686\n",
      "Trainable params: 56,686\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "print(\"Building model...\")\n",
    "model = build_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def evaluate(model):\n",
    "    y_prob = model.predict_proba(test_x, verbose=0)\n",
    "    y_pred = model.predict(test_x)\n",
    "    y_true = np.argmax(test_y, 1)\n",
    "\n",
    "    # evaluate the model\n",
    "    score, accuracy = model.evaluate(test_x, test_y, batch_size=32)\n",
    "    print(\"Accuracy = {:.2f}\".format(accuracy))\n",
    "    \n",
    "    return roc, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n",
      "Train on 308 samples, validate on 308 samples\n",
      "Epoch 1/3\n",
      "308/308 [==============================] - 1s 3ms/step - loss: 0.6860 - acc: 0.8030 - val_loss: 0.6518 - val_acc: 0.8041\n",
      "Epoch 2/3\n",
      "308/308 [==============================] - 1s 3ms/step - loss: 0.6869 - acc: 0.8117 - val_loss: 0.6548 - val_acc: 0.8285\n",
      "Epoch 3/3\n",
      "308/308 [==============================] - 1s 4ms/step - loss: 0.6662 - acc: 0.8111 - val_loss: 0.6527 - val_acc: 0.8009\n",
      "Evaluating model...\n",
      "308/308 [==============================] - 0s 733us/step\n",
      "Accuracy = 0.80\n"
     ]
    }
   ],
   "source": [
    "print(\"Training model...\")\n",
    "earlystop = EarlyStopping(monitor='val_loss', patience=3, verbose=1, mode='auto')\n",
    "model.fit(train_x, train_y, validation_data=(dev_x, dev_y), callbacks=[earlystop], batch_size=20, epochs=3)\n",
    "\n",
    "# now evaluate the trained model against the unseen test data\n",
    "print(\"Evaluating model...\")\n",
    "roc, acc = evaluate(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs229",
   "language": "python",
   "name": "cs229-final"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
